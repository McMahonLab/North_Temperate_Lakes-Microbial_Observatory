hits <- which(all.network$index1 == names(MAH.edges)[i] | all.network$index2 == names(MAH.edges)[i])
MAH.corr[i] <- mean(all.network$LSA[hits])
}
MAH.quant <- MAH.edges * MAH.corr
MAH.metric <- colSums(sweep(MAH.conn, 1, MAH.quant, "*"))
MAH.dates <- extract_date(colnames(MAH))
all.metric <- c(TBH.metric, NSH.metric, MAH.metric)
all.dates <- c(TBH.dates, NSH.dates, MAH.dates)
lakekey <- c(rep("TBH", length(TBH.metric)), rep("NSH", length(NSH.metric)), rep("MAH", length(MAH.metric)))
plot.conn <- data.frame(lakekey, all.dates, all.metric)
colnames(plot.conn) <- c("Lake", "Date", "Connectivity")
metalakes <- substr(metadata$Sample_Name, start=1, stop=3)
metayears <- substr(metadata$Sample_Name, start=9, stop=10)
metaTBH <- metadata[which(metalakes == "TBH"), c(1,2,3)]
metaTBH <- dcast(metaTBH, Sample_Name~Depth, fun.aggregate=mean)
metaNSH <- metadata[which(metalakes == "NSH"), c(1,2,3)]
metaNSH <- dcast(metaNSH, Sample_Name~Depth, fun.aggregate=mean)
metaMAH <- metadata[which(metalakes == "MAH"), c(1,2,3)]
metaMAH <- dcast(metaMAH, Sample_Name~Depth, fun.aggregate=mean)
TBHdates <- extract_date(metaTBH$Sample_Name)
NSHdates <- extract_date(metaNSH$Sample_Name)
MAHdates <- extract_date(metaMAH$Sample_Name)
cont.mixes <- c()
for(i in 1:dim(plot.conn)[1]){
if(plot.conn$Lake[i] == "TBH"){
sample <- metaTBH[which(TBHdates == plot.conn$Date[i]),]
sample <- sample[2:length(sample)]
sample <- as.numeric(sample[which(is.na(sample) == F)])
cont.mixes[i] <- max(sample) - min(sample)
}else if(plot.conn$Lake[i] == "NSH"){
sample <- metaNSH[which(NSHdates == plot.conn$Date[i]),]
sample <- sample[2:length(sample)]
sample <- as.numeric(sample[which(is.na(sample) == F)])
cont.mixes[i] <- max(sample) - min(sample)
}else if(plot.conn$Lake[i] == "MAH"){
sample <- metaMAH[which(MAHdates == plot.conn$Date[i]),]
sample <- sample[2:length(sample)]
sample <- as.numeric(sample[which(is.na(sample) == F)])
cont.mixes[i] <- max(sample) - min(sample)
}
}
cont.mixes
hist(cont.mixes)
cont.mixes[which(cont.mixes == -Inf)] <- NA
year <- substr(plot.conn$Date, start=1, stop=4)
which(plot.conn$Connectivity == -Inf)
plot.conn$Year<- factor(year, levels=c("2005", "2007", "2008", "2009"))
julian <- c()
for(i in 1:dim(plot.conn)[1]){
if(year[i] == "2005"){
julian[i] <- as.numeric(plot.conn$Date[i] - extract_date(c("TBH01Jun05")))
}else if(year[i] == "2007"){
julian[i] <- as.numeric(plot.conn$Date[i] - extract_date(c("TBH01Jun07")))
}else if(year[i] == "2008"){
julian[i] <- as.numeric(plot.conn$Date[i] - extract_date(c("TBH01Jun08")))
}else if(year[i] == "2009"){
julian[i] <- as.numeric(plot.conn$Date[i] - extract_date(c("TBH01Jun09")))
}
}
plot.conn$DayNum <- julian
i=1
plot.conn$Lake[i]
sample <- metaTBH[which(TBHdates == plot.conn$Date[i]),]
sample <- sample[2:length(sample)]
sample
max(sample) - min(sample)
sample <- as.numeric(sample[which(is.na(sample) == F)])
max(sample) - min(sample)
sample
plot.conn$Date[i]
plot.conn$Lake
i=171
sample <- metaNSH[which(NSHdates == plot.conn$Date[i]),]
sample <- sample[2:length(sample)]
sample
i=307
sample <- metaMAH[which(MAHdates == plot.conn$Date[i]),]
sample <- sample[2:length(sample)]
sample
cont.mixes <- c()
for(i in 1:dim(plot.conn)[1]){
if(plot.conn$Lake[i] == "TBH"){
sample <- metaTBH[which(TBHdates == plot.conn$Date[i]),]
sample <- sample[2:length(sample)]
sample <- as.numeric(sample[which(is.na(sample) == F)])
cont.mixes[i] <- sample[3] - min(sample)
}else if(plot.conn$Lake[i] == "NSH"){
sample <- metaNSH[which(NSHdates == plot.conn$Date[i]),]
sample <- sample[2:length(sample)]
sample <- as.numeric(sample[which(is.na(sample) == F)])
cont.mixes[i] <- sample[3] - min(sample)
}else if(plot.conn$Lake[i] == "MAH"){
sample <- metaMAH[which(MAHdates == plot.conn$Date[i]),]
sample <- sample[2:length(sample)]
sample <- as.numeric(sample[which(is.na(sample) == F)])
cont.mixes[i] <- sample[3] - min(sample)
}
}
which(cont.mixes == -Inf)
warnings(0)
warnings()
cont.mixes
i=359
plot.conn$Lake[i]
sample <- metaMAH[which(MAHdates == plot.conn$Date[i]),]
sample
plot.conn$Mixing.cont <- cont.mixes
year <- substr(plot.conn$Date, start=1, stop=4)
plot.conn$Year<- factor(year, levels=c("2005", "2007", "2008", "2009"))
julian <- c()
for(i in 1:dim(plot.conn)[1]){
if(year[i] == "2005"){
julian[i] <- as.numeric(plot.conn$Date[i] - extract_date(c("TBH01Jun05")))
}else if(year[i] == "2007"){
julian[i] <- as.numeric(plot.conn$Date[i] - extract_date(c("TBH01Jun07")))
}else if(year[i] == "2008"){
julian[i] <- as.numeric(plot.conn$Date[i] - extract_date(c("TBH01Jun08")))
}else if(year[i] == "2009"){
julian[i] <- as.numeric(plot.conn$Date[i] - extract_date(c("TBH01Jun09")))
}
}
plot.conn$DayNum <- julian
model <- lm(log(Connectivity+1) ~Year + Lake + Mixing.cont*DayNum, data=plot.conn[which(plot.conn$DayNum >= 0),])
summary(model)
summary(lm(log(Connectivity+1) ~Year + Lake + Mixing.cont+DayNum, data=plot.conn[which(plot.conn$DayNum >= 0),]))
plot(resid(model) ~ fitted(model))
plot(fitted(model)[which(plot.conn$DayNum >= 0 & plot.conn$Lake == "MAH")] ~ plot.conn$DayNum[which(plot.conn$DayNum >= 0 & plot.conn$Lake == "MAH")])
plot(fitted(model)[which(plot.conn$DayNum >= 0 & plot.conn$Lake == "TBH" & plot.conn$Mixing.cont < 7)] ~ plot.conn$DayNum[which(plot.conn$DayNum >= 0 & plot.conn$Lake == "TBH" & plot.conn$Mixing.cont < 7)])
plot(fitted(model)[which(plot.conn$DayNum >= 0 & plot.conn$Lake == "TBH" & plot.conn$Mixing.cont > 7)] ~ plot.conn$DayNum[which(plot.conn$DayNum >= 0 & plot.conn$Lake == "TBH" & plot.conn$Mixing.cont > 7)])
hist(plot.conn$Mixing.cont)
if(year[i] == "2005"){
q
plot(plot.conn$Date[which(plot.conn$Lake == "TBH")], plot.conn$Mixing.cont[which(plot.conn$Lake == "TBH")])
plot(plot.conn$DayNum[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont > 2 )], plot.conn$Mixing.cont[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont > 2 ))
plot(plot.conn$DayNum[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont > 2 )], plot.conn$Mixing.cont[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont > 2 ]))
plot(plot.conn$DayNum[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont > 2 )], plot.conn$Mixing.cont[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont > 2 )])
plot(plot.conn$DayNum[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont < 2 )], plot.conn$Mixing.cont[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont < 2 )])
plot(plot.conn$DayNum[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont > 5 )], plot.conn$Mixing.cont[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont > 5 )])
plot(plot.conn$DayNum[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont < 5 )], plot.conn$Mixing.cont[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont < 5 )])
plot(fitted(model)[which(plot.conn$DayNum >= 0 & plot.conn$Lake == "MAH")] ~ plot.conn$DayNum[which(plot.conn$DayNum >= 0 & plot.conn$Lake == "MAH")])
plot(plot.conn$DayNum[which(plot.conn$Lake == "NSH" & plot.conn$Mixing.cont > 5 )], plot.conn$Mixing.cont[which(plot.conn$Lake == "NSH" & plot.conn$Mixing.cont > 5 )])
plot(plot.conn$DayNum[which(plot.conn$Lake == "NSH" & plot.conn$Mixing.cont < 5 )], plot.conn$Mixing.cont[which(plot.conn$Lake == "NSH" & plot.conn$Mixing.cont < 5 )])
plot(plot.conn$DayNum[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont > 5 )], plot.conn$Connectivity[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont > 5 )])
plot(plot.conn$DayNum[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont < 5 )], plot.conn$Connectivity[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont < 5 )])
plot(plot.conn$DayNum[which(plot.conn$Lake == "NSH" & plot.conn$Mixing.cont > 5 )], plot.conn$Connectivity[which(plot.conn$Lake == "NSH" & plot.conn$Mixing.cont > 5 )])
plot(plot.conn$DayNum[which(plot.conn$Lake == "NSH" & plot.conn$Mixing.cont < 5 )], plot.conn$Connectivity[which(plot.conn$Lake == "NSH" & plot.conn$Mixing.cont < 5 )])
plot(fitted(model)[which(plot.conn$DayNum >= 0 & plot.conn$Lake == "MAH")] ~ plot.conn$DayNum[which(plot.conn$DayNum >= 0 & plot.conn$Lake == "MAH")])
plot(plot.conn$DayNum[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont > 5 )], plot.conn$Connectivity[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont > 5 )])
plot(plot.conn$DayNum[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont < 5 )], plot.conn$Connectivity[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont < 5 )])
plot(plot.conn$DayNum[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont > 3 )], plot.conn$Connectivity[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont > 3 )])
plot(plot.conn$DayNum[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont < 3 )], plot.conn$Connectivity[which(plot.conn$Lake == "TBH" & plot.conn$Mixing.cont < 3 )])
plot(plot.conn$DayNum[which(plot.conn$Lake == "NSH" & plot.conn$Mixing.cont > 3 )], plot.conn$Connectivity[which(plot.conn$Lake == "NSH" & plot.conn$Mixing.cont > 3 )])
plot(plot.conn$DayNum[which(plot.conn$Lake == "NSH" & plot.conn$Mixing.cont < 3 )], plot.conn$Connectivity[which(plot.conn$Lake == "NSH" & plot.conn$Mixing.cont < 3 )])
all.metric <- c(TBH.metric, NSH.metric, MAH.metric, SSH.metric, CBH.metric)
SSH.nodes <- match(names(all.edges), rownames(SSH))
SSH.conn <- SSH[SSH.nodes,]
SSH.edges <- all.edges[match(rownames(SSH.conn), names(all.edges))]
SSH.corr <- c()
for(i in 1:length(SSH.edges)){
hits <- which(all.network$index1 == names(SSH.edges)[i] | all.network$index2 == names(SSH.edges)[i])
SSH.corr[i] <- mean(all.network$LSA[hits])
}
SSH.quant <- SSH.edges * SSH.corr
SSH.metric <- colSums(sweep(SSH.conn, 1, SSH.quant, "*"))
SSH.dates <- extract_date(colnames(SSH))
CBH.nodes <- match(names(all.edges), rownames(CBH))
CBH.conn <- CBH[CBH.nodes,]
CBH.edges <- all.edges[match(rownames(CBH.conn), names(all.edges))]
CBH.corr <- c()
for(i in 1:length(CBH.edges)){
hits <- which(all.network$index1 == names(CBH.edges)[i] | all.network$index2 == names(CBH.edges)[i])
CBH.corr[i] <- mean(all.network$LSA[hits])
}
CBH.quant <- CBH.edges * CBH.corr
CBH.metric <- colSums(sweep(CBH.conn, 1, CBH.quant, "*"))
CBH.dates <- extract_date(colnames(CBH))
SSH <- bog_subset("SSH", otu_table)
CBH <- bog_subset("CBH", otu_table)
SSH.nodes <- match(names(all.edges), rownames(SSH))
SSH.conn <- SSH[SSH.nodes,]
SSH.edges <- all.edges[match(rownames(SSH.conn), names(all.edges))]
SSH.corr <- c()
for(i in 1:length(SSH.edges)){
hits <- which(all.network$index1 == names(SSH.edges)[i] | all.network$index2 == names(SSH.edges)[i])
SSH.corr[i] <- mean(all.network$LSA[hits])
}
SSH.quant <- SSH.edges * SSH.corr
SSH.metric <- colSums(sweep(SSH.conn, 1, SSH.quant, "*"))
SSH.dates <- extract_date(colnames(SSH))
CBH.nodes <- match(names(all.edges), rownames(CBH))
CBH.conn <- CBH[CBH.nodes,]
CBH.edges <- all.edges[match(rownames(CBH.conn), names(all.edges))]
CBH.corr <- c()
for(i in 1:length(CBH.edges)){
hits <- which(all.network$index1 == names(CBH.edges)[i] | all.network$index2 == names(CBH.edges)[i])
CBH.corr[i] <- mean(all.network$LSA[hits])
}
CBH.quant <- CBH.edges * CBH.corr
CBH.metric <- colSums(sweep(CBH.conn, 1, CBH.quant, "*"))
CBH.dates <- extract_date(colnames(CBH))
all.metric <- c(TBH.metric, NSH.metric, MAH.metric, SSH.metric, CBH.metric)
all.dates <- c(TBH.dates, NSH.dates, MAH.dates, SSH.dates, CBH.dates)
lakekey <- c(rep("TBH", length(TBH.metric)), rep("NSH", length(NSH.metric)), rep("MAH", length(MAH.metric)), rep("SSH", length(SSH.metric)), rep("CBH", length(CBH.metric)))
plot.conn <- data.frame(lakekey, all.dates, all.metric)
colnames(plot.conn) <- c("Lake", "Date", "Connectivity")
ggplot(data = plot.conn, aes(x = Date, y = Connectivity, colour = Lake)) + geom_line(size = 1) + scale_y_log10() + coord_cartesian(xlim = extract_date(c("TBH15May07", "TBH18Nov07"))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = "black"), axis.ticks = element_line(colour = "black")) + theme(axis.text.x = element_text(hjust = 0.5, size = 10, colour = "black"), axis.title.x = element_text(size = 12, vjust = 0.3), axis.title.y = element_text(size=12, vjust=1.3), axis.text.y = element_text(colour = "black", size = 10))
ggplot(data = plot.conn, aes(x = Date, y = Connectivity, colour = Lake)) + geom_line(size = 1) + scale_y_log10() + coord_cartesian(xlim = extract_date(c("TBH15May08", "TBH18Nov08"))) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = "black"), axis.ticks = element_line(colour = "black")) + theme(axis.text.x = element_text(hjust = 0.5, size = 10, colour = "black"), axis.title.x = element_text(size = 12, vjust = 0.3), axis.title.y = element_text(size=12, vjust=1.3), axis.text.y = element_text(colour = "black", size = 10))
all.network <- read.table(file = "C:/Users/Alex/Desktop/North_Temperate_Lakes-Microbial_Observatory/Network_analysis/allsamples_network_28Jan16.txt", header = T)
TBH <- bog_subset("TBH", otu_table)
NSH <- bog_subset("NSH", otu_table)
MAH <- bog_subset("MAH", otu_table)
SSH <- bog_subset("SSH", otu_table)
CBH <- bog_subset("CBH", otu_table)
all.edges <- table(c(as.character(all.network$index1), as.character(all.network$index2)))
# Calcuate connectivity metric for each sample by lake
# metric (for each sample) = sum(OTU abundance * # of connections to that OTU * average strength of connections to that OTU)
TBH.nodes <- match(names(all.edges), rownames(TBH))
TBH.conn <- TBH[TBH.nodes,]
TBH.edges <- all.edges[match(rownames(TBH.conn), names(all.edges))]
TBH.corr <- c()
for(i in 1:length(TBH.edges)){
hits <- which(all.network$index1 == names(TBH.edges)[i] | all.network$index2 == names(TBH.edges)[i])
TBH.corr[i] <- mean(all.network$LSA[hits])
}
TBH.quant <- TBH.edges * TBH.corr
TBH.metric <- colSums(sweep(TBH.conn, 1, TBH.quant, "*"))
TBH.dates <- extract_date(colnames(TBH))
NSH.nodes <- match(names(all.edges), rownames(NSH))
NSH.conn <- NSH[NSH.nodes,]
NSH.edges <- all.edges[match(rownames(NSH.conn), names(all.edges))]
NSH.corr <- c()
for(i in 1:length(NSH.edges)){
hits <- which(all.network$index1 == names(NSH.edges)[i] | all.network$index2 == names(NSH.edges)[i])
NSH.corr[i] <- mean(all.network$LSA[hits])
}
NSH.quant <- NSH.edges * NSH.corr
NSH.metric <- colSums(sweep(NSH.conn, 1, NSH.quant, "*"))
NSH.dates <- extract_date(colnames(NSH))
MAH.nodes <- match(names(all.edges), rownames(MAH))
MAH.conn <- MAH[MAH.nodes,]
MAH.edges <- all.edges[match(rownames(MAH.conn), names(all.edges))]
MAH.corr <- c()
for(i in 1:length(MAH.edges)){
hits <- which(all.network$index1 == names(MAH.edges)[i] | all.network$index2 == names(MAH.edges)[i])
MAH.corr[i] <- mean(all.network$LSA[hits])
}
MAH.quant <- MAH.edges * MAH.corr
MAH.metric <- colSums(sweep(MAH.conn, 1, MAH.quant, "*"))
MAH.dates <- extract_date(colnames(MAH))
CBH.nodes <- match(names(all.edges), rownames(CBH))
CBH.conn <- CBH[CBH.nodes,]
CBH.edges <- all.edges[match(rownames(CBH.conn), names(all.edges))]
CBH.corr <- c()
for(i in 1:length(CBH.edges)){
hits <- which(all.network$index1 == names(CBH.edges)[i] | all.network$index2 == names(CBH.edges)[i])
CBH.corr[i] <- mean(all.network$LSA[hits])
}
CBH.quant <- CBH.edges * CBH.corr
CBH.metric <- colSums(sweep(CBH.conn, 1, CBH.quant, "*"))
CBH.dates <- extract_date(colnames(CBH))
SSH.nodes <- match(names(all.edges), rownames(SSH))
SSH.conn <- SSH[SSH.nodes,]
SSH.edges <- all.edges[match(rownames(SSH.conn), names(all.edges))]
SSH.corr <- c()
for(i in 1:length(SSH.edges)){
hits <- which(all.network$index1 == names(SSH.edges)[i] | all.network$index2 == names(SSH.edges)[i])
SSH.corr[i] <- mean(all.network$LSA[hits])
}
SSH.quant <- SSH.edges * SSH.corr
SSH.metric <- colSums(sweep(SSH.conn, 1, SSH.quant, "*"))
SSH.dates <- extract_date(colnames(SSH))
all.metric <- c(TBH.metric, NSH.metric, MAH.metric, CBH.metric, SSH.metric)
all.dates <- c(TBH.dates, NSH.dates, MAH.dates, CBH.dates, SSH.dates)
lakekey <- c(rep("TBH", length(TBH.metric)), rep("NSH", length(NSH.metric)), rep("MAH", length(MAH.metric)), rep("CBH", length(CBH.metric)), rep("SSH", length(SSH.metric)))
plot.conn <- data.frame(lakekey, all.dates, all.metric)
colnames(plot.conn) <- c("Lake", "Date", "Connectivity")
plot.conn$Lake <- factor(plot.conn$Lake, levels = c("CBH", "NSH", "TBH", "SSH", "MAH"))
ggplot(data = plot.conn, aes(x = Lake, y = Connectivity)) + geom_boxplot() + scale_y_log10() + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_blank(), axis.line = element_line(colour = "black"), axis.text = element_text(colour = "black"))
#Test significant differences
lakeids <- levels(plot.conn$Lake)
Lake1 <- character(0)
Lake2 <- character(0)
Test <- character(0)
pvalue <- numeric(0)
Interpretation <- character(0)
for (id in lakeids) {
x = subset(plot.conn, plot.conn$Lake == id)
for (lake in lakeids) {
y = subset(plot.conn, plot.conn$Lake == lake)
result <- wilcox.exact(x$Connectivity,y$Connectivity, alternative= "two.sided", conf.level = 0.95)
Lake1 <- c(Lake1, id)
Lake2 <- c(Lake2, lake)
Test <- c(Test, result$method)
pvalue <- c(pvalue, result$p.value)
if (result$p.value < 0.05) {
Interpretation <- c(Interpretation, "Reject Null (difference in median detected)")
} else {
Interpretation <- c(Interpretation, "Accept Null (difference in median undetected)")
}
}
}
Wilcoxon.conn <- data.frame(Lake1, Lake2, Test, pvalue, Interpretation)
#print(Wilcoxon.conn)
print(Wilcoxon.conn)
library(vegan)        # Used for Bray-Curtis
library(ggplot2)      # Used for plotting
library(reshape2)     # Used to format metadata
clade_table <- combine_otus("Clade", otu_table, taxonomy)
data(taxonomy)
clade_table <- combine_otus("Clade", otu_table, taxonomy)
clade_table <- reduce_names(clade_table)
clade_table07 <- year_subset("07", clade_table)
bog1 <- "TBH"
bog2 <- "MAH"
table <- clade_table07
query <- bog_subset(bog1, table)
database <- bog_subset(bog2, table)
# Remove January samples
query <- query[, c(1:32, 35:80)]
# Calculate Bray-Curtis Similarity between every dimictic and meromictic sample in the given subset, then average by dimictic sample
mean.bc <- c()
for (i in 1:dim(query)[2]) {
bc.dis <- c()
for (j in 1:dim(database)[2]) {
test <- cbind(query[, i], database[, j])
# Subtract from 1 to convert dissimilarity to similarity
bc.dis[j] <- 1 - vegdist(t(test), method = "bray")
}
mean.bc[i] <- mean(bc.dis)
}
# Make dataframe for plotting
dates <- extract_date(colnames(query))
plot.data <- data.frame(dates, mean.bc)
colnames(plot.data) <- c("Dates", "BrayCurtis")
TBHmat <- make_temp_matrix("TBH.....07", metadata)
TBHmat <- melt(TBHmat)
colnames(TBHmat) <- c("Depth", "Date", "Temperature")
TBHmat$Date <- as.Date(TBHmat$Date, format = "%Y-%m-%d")
TBHmat$Depth <- -TBHmat$Depth / 18.04 + 0.388
# Need to close polygons - add 0 or max values at top and bottom of graph
TBHmat <- TBHmat[which(is.na(TBHmat$Temperature) == F), ]
# For each date, add a hidden value outside of the plotting range
add <- TBHmat[which(TBHmat$Depth == 0.388), ]
add$Depth <- rep(-1, length(add$Depth))
add$Temperature <- rep(4, length(add$Temperature))
add2 <- add
add2$Depth <- rep(3, length(add2$Depth))
add2$Temperature <- rep(28, length(add2$Temperature))
TBHmat2 <- rbind(TBHmat, add, add2)
pdf(file = paste(path2repo, "TBH_v_MAH_bray_curtis.pdf", sep = ""), width = 3.3125, height = 2.5)
ggplot() + stat_contour(data = TBHmat2, aes(y = Depth, x = Date, z = Temperature, fill = ..level..), geom = "polygon") + scale_fill_gradientn(colours = c("dodgerblue", "cyan", "green", "yellow", "red"), "Temp", limits = c(4, 28)) + geom_line(data = plot.data, aes(x = Dates, y = BrayCurtis), size = 1.5) + labs(y = "Sorenson Similarity Index", x = NULL) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_rect(fill = "dodgerblue3"), axis.line = element_line(colour = "black"), axis.ticks = element_line(colour="black")) + theme(axis.text.x = element_text(hjust = 0.5, size = 12, colour = "black"), axis.title.x = element_text(size = 15, vjust = 0.2), axis.title.y = element_text(size = 15, vjust = 1.6), axis.text.y = element_text(colour = "black", size = 10)) + coord_cartesian(xlim = extract_date(c("TBH07Jun07", "TBH11Nov07")), ylim = c(0.07, 0.372))
dev.off()
path2repo <- "C:/Users/Alex/Desktop/North_Temperate_Lakes-Microbial_Observatory/Figures/"
# Calculate Bray-Curtis Similarity between every dimictic and meromictic sample in the given subset, then average by dimictic sample
mean.bc <- c()
for (i in 1:dim(query)[2]) {
bc.dis <- c()
for (j in 1:dim(database)[2]) {
test <- cbind(query[, i], database[, j])
# Subtract from 1 to convert dissimilarity to similarity
bc.dis[j] <- 1 - vegdist(t(test), method = "bray")
}
mean.bc[i] <- mean(bc.dis)
}
# Make dataframe for plotting
dates <- extract_date(colnames(query))
plot.data <- data.frame(dates, mean.bc)
colnames(plot.data) <- c("Dates", "BrayCurtis")
TBHmat <- make_temp_matrix("TBH.....07", metadata)
TBHmat <- melt(TBHmat)
colnames(TBHmat) <- c("Depth", "Date", "Temperature")
TBHmat$Date <- as.Date(TBHmat$Date, format = "%Y-%m-%d")
TBHmat$Depth <- -TBHmat$Depth / 18.04 + 0.388
# Need to close polygons - add 0 or max values at top and bottom of graph
TBHmat <- TBHmat[which(is.na(TBHmat$Temperature) == F), ]
# For each date, add a hidden value outside of the plotting range
add <- TBHmat[which(TBHmat$Depth == 0.388), ]
add$Depth <- rep(-1, length(add$Depth))
add$Temperature <- rep(4, length(add$Temperature))
add2 <- add
add2$Depth <- rep(3, length(add2$Depth))
add2$Temperature <- rep(28, length(add2$Temperature))
TBHmat2 <- rbind(TBHmat, add, add2)
pdf(file = paste(path2repo, "TBH_v_MAH_bray_curtis.pdf", sep = ""), width = 3.3125, height = 2.5)
ggplot() + stat_contour(data = TBHmat2, aes(y = Depth, x = Date, z = Temperature, fill = ..level..), geom = "polygon") + scale_fill_gradientn(colours = c("dodgerblue", "cyan", "green", "yellow", "red"), "Temp", limits = c(4, 28)) + geom_line(data = plot.data, aes(x = Dates, y = BrayCurtis), size = 1.5) + labs(y = "Sorenson Similarity Index", x = NULL) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_rect(fill = "dodgerblue3"), axis.line = element_line(colour = "black"), axis.ticks = element_line(colour="black")) + theme(axis.text.x = element_text(hjust = 0.5, size = 12, colour = "black"), axis.title.x = element_text(size = 15, vjust = 0.2), axis.title.y = element_text(size = 15, vjust = 1.6), axis.text.y = element_text(colour = "black", size = 10)) + coord_cartesian(xlim = extract_date(c("TBH07Jun07", "TBH11Nov07")), ylim = c(0.07, 0.372))
dev.off()
clade_table08 <- year_subset("08", clade_table)
bog1 <- "NSH"
bog2 <- "MAH"
table <- clade_table08
query <- bog_subset(bog1, table)
database <- bog_subset(bog2, table)
# Calculate Bray-Curtis Similarity between every dimictic and meromictic sample in the given subset, then average by dimictic sample
mean.bc <- c()
for(i in 1:dim(query)[2]){
bc.dis <- c()
for(j in 1:dim(database)[2]){
test <- cbind(query[, i], database[, j])
#Subtract from 1 to convert dissimilarity to similarity
bc.dis[j] <- 1 - vegdist(t(test), method="bray")
}
mean.bc[i] <- mean(bc.dis)
}
# Make dataframe for plotting
dates <- extract_date(colnames(query))
plot.data <- data.frame(dates, mean.bc)
colnames(plot.data) <- c("Dates", "BrayCurtis")
NSHmat <- make_temp_matrix("NSH.....08", metadata)
# Remove columns with NAs
NSHmat <- NSHmat[,c(1:14, 16:30, 32:54)]
NSHmat <- melt(NSHmat)
colnames(NSHmat) <- c("Depth", "Date", "Temperature")
NSHmat$Date <- as.Date(NSHmat$Date, format = "%Y-%m-%d")
NSHmat$Depth <- -NSHmat$Depth / 10.71 + 0.43
# Need to close polygons - add 0 or max values at top and bottom of graph
NSHmat <- NSHmat[which(is.na(NSHmat$Temperature) == F), ]
# For each date, add a hidden -1 value
add <- NSHmat[which(NSHmat$Depth == 0.43),]
add$Depth <- rep(-1, length(add$Depth))
add$Temperature <- rep(4, length(add$Temperature))
add2 <- add
add2$Depth <- rep(3, length(add2$Depth))
add2$Temperature <- rep(28, length(add2$Temperature))
NSHmat2 <- rbind(NSHmat, add, add2)
# Remove dates with missing depth measurements
pdf(file = paste(path2repo, "NSH_v_MAH_bray_curtis.pdf", sep = ""), width = 3.3125, height = 2.5)
ggplot() + stat_contour(data = NSHmat2, aes(y = Depth, x = Date, z = Temperature, fill = ..level..), geom = "polygon") + coord_cartesian(xlim = extract_date(c("NSH25May08", "NSH01Nov08")), ylim = c(0.05, 0.42)) + scale_fill_gradientn(colours = c("dodgerblue", "cyan", "green", "yellow", "red"), "Temp", limits = c(4, 28)) + geom_line(data = plot.data, aes(x = Dates, y = BrayCurtis), size = 1.5) + labs(y = "Sorenson Similarity Index", x = NULL) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_rect(fill="dodgerblue3"), axis.line = element_line(colour = "black"), axis.ticks = element_line(colour="black")) + theme(axis.text.x = element_text(hjust = 0.5, size = 12, colour = "black"), axis.title.x = element_text(size = 15, vjust = 0.2), axis.title.y = element_text(size = 15, vjust = 1.6), axis.text.y = element_text(colour = "black", size = 10))
dev.off()
# Select data
bog1 <- "TBH"
bog2 <- "MAH"
table <- clade_table07
query <- bog_subset(bog1, table)
database <- bog_subset(bog2, table)
# Remove January samples
query <- query[, c(1:32, 35:80)]
# Calculate Bray-Curtis Similarity between every dimictic and meromictic sample in the given subset, then average by dimictic sample
mean.bc <- c()
for (i in 1:dim(query)[2]) {
bc.dis <- c()
for (j in 1:dim(database)[2]) {
test <- cbind(query[, i], database[, j])
# Subtract from 1 to convert dissimilarity to similarity
bc.dis[j] <- 1 - vegdist(t(test), method = "bray")
}
mean.bc[i] <- mean(bc.dis)
}
# Make dataframe for plotting
dates <- extract_date(colnames(query))
plot.data <- data.frame(dates, mean.bc)
colnames(plot.data) <- c("Dates", "BrayCurtis")
TBHmat <- make_temp_matrix("TBH.....07", metadata)
TBHmat <- melt(TBHmat)
colnames(TBHmat) <- c("Depth", "Date", "Temperature")
TBHmat$Date <- as.Date(TBHmat$Date, format = "%Y-%m-%d")
TBHmat$Depth <- -TBHmat$Depth / 18.04 + 0.388
# Need to close polygons - add 0 or max values at top and bottom of graph
TBHmat <- TBHmat[which(is.na(TBHmat$Temperature) == F), ]
# For each date, add a hidden value outside of the plotting range
add <- TBHmat[which(TBHmat$Depth == 0.388), ]
add$Depth <- rep(-1, length(add$Depth))
add$Temperature <- rep(4, length(add$Temperature))
add2 <- add
add2$Depth <- rep(3, length(add2$Depth))
add2$Temperature <- rep(28, length(add2$Temperature))
TBHmat2 <- rbind(TBHmat, add, add2)
pdf(file = paste(path2repo, "TBH_v_MAH_bray_curtis.pdf", sep = ""), width = 3.3125, height = 2.5)
ggplot() + stat_contour(data = TBHmat2, aes(y = Depth, x = Date, z = Temperature, fill = ..level..), geom = "polygon") + scale_fill_gradientn(colours = c("dodgerblue", "cyan", "green", "yellow", "red"), "Temp", limits = c(4, 28)) + geom_line(data = plot.data, aes(x = Dates, y = BrayCurtis), size = 1.5) + labs(y = "Sorenson Similarity Index", x = NULL) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_rect(fill = "dodgerblue3"), axis.line = element_line(colour = "black"), axis.ticks = element_line(colour="black")) + theme(axis.text.x = element_text(hjust = 0.5, size = 12, colour = "black"), axis.title.x = element_text(size = 15, vjust = 0.2), axis.title.y = element_text(size = 12, vjust = 1.6), axis.text.y = element_text(colour = "black", size = 10)) + coord_cartesian(xlim = extract_date(c("TBH07Jun07", "TBH11Nov07")), ylim = c(0.07, 0.372))
dev.off()
# Select data
clade_table08 <- year_subset("08", clade_table)
bog1 <- "NSH"
bog2 <- "MAH"
table <- clade_table08
query <- bog_subset(bog1, table)
database <- bog_subset(bog2, table)
# Calculate Bray-Curtis Similarity between every dimictic and meromictic sample in the given subset, then average by dimictic sample
mean.bc <- c()
for(i in 1:dim(query)[2]){
bc.dis <- c()
for(j in 1:dim(database)[2]){
test <- cbind(query[, i], database[, j])
#Subtract from 1 to convert dissimilarity to similarity
bc.dis[j] <- 1 - vegdist(t(test), method="bray")
}
mean.bc[i] <- mean(bc.dis)
}
# Make dataframe for plotting
dates <- extract_date(colnames(query))
plot.data <- data.frame(dates, mean.bc)
colnames(plot.data) <- c("Dates", "BrayCurtis")
NSHmat <- make_temp_matrix("NSH.....08", metadata)
# Remove columns with NAs
NSHmat <- NSHmat[,c(1:14, 16:30, 32:54)]
NSHmat <- melt(NSHmat)
colnames(NSHmat) <- c("Depth", "Date", "Temperature")
NSHmat$Date <- as.Date(NSHmat$Date, format = "%Y-%m-%d")
NSHmat$Depth <- -NSHmat$Depth / 10.71 + 0.43
# Need to close polygons - add 0 or max values at top and bottom of graph
NSHmat <- NSHmat[which(is.na(NSHmat$Temperature) == F), ]
# For each date, add a hidden -1 value
add <- NSHmat[which(NSHmat$Depth == 0.43),]
add$Depth <- rep(-1, length(add$Depth))
add$Temperature <- rep(4, length(add$Temperature))
add2 <- add
add2$Depth <- rep(3, length(add2$Depth))
add2$Temperature <- rep(28, length(add2$Temperature))
NSHmat2 <- rbind(NSHmat, add, add2)
# Remove dates with missing depth measurements
pdf(file = paste(path2repo, "NSH_v_MAH_bray_curtis.pdf", sep = ""), width = 3.3125, height = 2.5)
ggplot() + stat_contour(data = NSHmat2, aes(y = Depth, x = Date, z = Temperature, fill = ..level..), geom = "polygon") + coord_cartesian(xlim = extract_date(c("NSH25May08", "NSH01Nov08")), ylim = c(0.05, 0.42)) + scale_fill_gradientn(colours = c("dodgerblue", "cyan", "green", "yellow", "red"), "Temp", limits = c(4, 28)) + geom_line(data = plot.data, aes(x = Dates, y = BrayCurtis), size = 1.5) + labs(y = "Sorenson Similarity Index", x = NULL) + theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(), panel.background = element_rect(fill="dodgerblue3"), axis.line = element_line(colour = "black"), axis.ticks = element_line(colour="black")) + theme(axis.text.x = element_text(hjust = 0.5, size = 12, colour = "black"), axis.title.x = element_text(size = 15, vjust = 0.2), axis.title.y = element_text(size = 12, vjust = 1.6), axis.text.y = element_text(colour = "black", size = 10))
dev.off()
